---
title: "Missing Data Imputation"
author: "Dominik Glandorf"
date: "2023-03-01"
output: pdf_document
---

```{r, include=FALSE, warning=FALSE, message=FALSE}
if(!require(mice)) install.packages('mice')
if(!require(corrplot)) install.packages('corrplot')
setwd("~/EWS")
source("read_data.R")
data = get_aggregated_features()
```

# Problem statement
Due to the large amount of missing/non-applicable data in our dataset (e.g. 44.3% of students without any AP exams), keeping only complete observations is impractical because it would leave us with a tiny fraction of the whole set and is likely to bias the drawn conclusions if data is not missing completely at random.

## Univariate missingness
```{r} 
missing_per_feature = data.frame(
  feature=names(data),
  mean_na=colMeans(is.na(data)),
  type=sapply(data, typeof))

ggplot(missing_per_feature, aes(x=reorder(feature,mean_na),
                                y=mean_na,
                                fill=type=="double")) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(title="Missing data by feature", x="Feature", y="Amount missing", fill="Continuous") +
  scale_y_continuous(labels = scales::percent) +
  coord_flip()
```
- IELTS score should be dropped
```{r}
data = data[,names(data)!='ielts_score']
```
- TOEFL score? Maybe convert into: toefl reported, check correlation with int student
```{r}
cor(data$int_student, !is.na(data$toefl_score), use="complete.obs")
toefl_dropout = aggregate(data %>% filter(int_student==1), dropout ~ round(toefl_score/5), FUN=mean)
toefl_nr = aggregate(data %>% filter(int_student==1), dropout ~ round(toefl_score/5), FUN=length)
toefl_dropout$nr = toefl_nr$dropout
names(toefl_dropout) = c("score","dropout")
ggplot(toefl_dropout, aes(x=score, y=dropout, fill=nr)) + geom_bar(stat="identity")
```

- Sports at admission has an unclear interpretation anyway
```{r}
table(data$sport_at_admission)
```
- make binary predictor
- Advanced Placement exams are reported for roughly every other student
  combine it into categorical, no imputation
- remove cohorts with large amount of missing data

```{r}
missing_per_student = rowSums(is.na(data))
ggplot() + aes(missing_per_student) + geom_bar() + labs(title="Number of missing features per student", x="", y="")
```

# Cohorts between 2008 and 2011
```{r}
students = get_student_sub()
data = data %>% left_join(students %>% select(mellon_id, first_year))
data_sub = data %>% filter(first_year <= 2010)

missing_per_feature = data.frame(
  feature=names(data_sub),
  mean_na=colMeans(is.na(data_sub)),
  type=sapply(data_sub, typeof))

ggplot(missing_per_feature, aes(x=reorder(feature,mean_na),
                                y=mean_na,
                                fill=type=="double")) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(title="Missing data by feature", x="Feature", y="Amount missing", fill="Continuous") +
  scale_y_continuous(labels = scales::percent)
```


## Bivariate missingness
```{r}
na_biv = cor(is.na(data[,colSums(is.na(data))>0]))
corrplot(na_biv)
na_biv[lower.tri(na_biv, diag=T)] = NA
```
Missing value prediction may be hard for the clusters where multiple variables are missing.

```{r}
library(VIM)
marginplot(data[,c("uc_math_score","hs_gpa")])
```


## Frequent patterns of missing data
```{r}
md.pattern(data)
```

```{r}
md.pattern(data[,c("sport_at_admission", "hs_gpa", "uc_math_score","best_ap")], rotate.names = T)
```

```{r}
aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

# Solution strategy
Simple strategies just do single and univariate imputation (see below).

Better: Train a model that recursively predicts NA values from other predictors -> MICE, missMDA. Downside: Increases dependencies between predictors.

Simple strategies for categorical variables
1. Own category for NA: We add NA values as a dummy variable (and the reference category).
2. Use the most frequent class: We merge them with the most frequent class, save one parameter but also bias the distribution to its mode.

Simple strategies for continuous variables
1. Discretize and use the same strategy as for continuous variables.
2. Use mean.
3. Use random value.

# MICE
MICE does iterative multivariate imputation using a model such as random forest.
```{r}
# parameters of mice
# m: the number of imputed datasets
# maxit: the number of iterations in each imputation
# meth: imputation method = random forest
system.time(tempData <- mice(data, m=3, maxit=3, meth='rf', seed=500))
```
# Diagnose imputation
```{r}
densityplot(tempData)
```

# Old strategy tests
This is an outdated implementation of different simple imputation strategies and testing their impact on the prediction.

```{r}
compute_AUC = function(dat,i,CV_folds) {
  train = dat[1:nrow(dat)%%CV_folds!=i,]
  test = dat[1:nrow(dat)%%CV_folds==i,]
  log.reg.fit = glm(dropout ~ ., train, family = 'binomial')
  dropout_prob = predict(log.reg.fit, test, type = "response")
  return(suppressMessages(auc(test$dropout, dropout_prob)))
}

mode = function(vector) tail(names(sort(table(vector))), 1)

AUC_categorical = function(dat, predictor, NA_strategy, CV_folds=5) {
  dat = dat %>% select("dropout", feature=predictor) %>% mutate(feature=as.factor(feature))
  
  if (NA_strategy == "mode") {
    dat = dat %>% replace_na(list(feature=mode(dat$feature)))
  }
  if (NA_strategy == "new_level") {
    dat = dat %>% mutate(feature = fct_explicit_na(feature, "unknown"))
  }

  return(round(mean(sapply((1:CV_folds)-1, function(i) compute_AUC(dat,i,CV_folds))),3))
}

plot_AUCs = function(AUCs) {
  AUCs$variable = rownames(AUCs)
  AUCs.long = AUCs %>% pivot_longer(cols = head(names(AUCs), ncol(AUCs)-1), names_to="method", values_to="value")
  ggplot(AUCs.long, aes(x=variable, y=value, fill=method)) +
    geom_bar(stat = "identity", position = "dodge") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(x="Predictor", y="AUC", fill="NA strategy") +
    coord_cartesian(ylim = c(0.5,1))
}
```

```{r}
get_aucs = function(method) sapply(categorical, function(c) AUC_categorical(students, c, method))
aucs_cat = data.frame(none=get_aucs(""), mode=get_aucs("mode"), new_level=get_aucs("new_level"))
plot_AUCs(aucs_cat)
```

```{r}
AUC_continuous = function(dat, predictor, NA_strategy, CV_folds=5) {
  dat = dat %>% select("dropout", feature=predictor)

  if (NA_strategy == "mean") {
    dat = dat %>% replace_na(list(feature=mean(dat$feature)))
  }
  
  if (NA_strategy == "discretize5") {
   quantiles = quantile(dat$feature, probs = seq(0, 1, 0.2), na.rm=T) # 5 equally sized quantiles
   quantiles = quantiles + 1:6 * 0.0001 # to prevent an error for unequally distr variables
   dat = dat %>% 
     mutate(feature = cut(feature, breaks = quantiles, include.lowest=T, labels=F)) %>%
      mutate(feature = as.factor(feature)) %>%
      mutate(feature = fct_explicit_na(feature, "unknown"))
  }
  
  if (NA_strategy == "random") {
    dat = dat %>% mutate(feature = ifelse(is.na(feature), sample(feature[!is.na(feature)], 1), feature))
  }
  
  return(round(mean(sapply((1:CV_folds)-1, function(i) compute_AUC(dat,i,CV_folds))),3))
}

get_aucs = function(method) sapply(continuous, function(c) AUC_continuous(students, c, method))
aucs = data.frame(none=get_aucs(""), mean=get_aucs("mean"), discretize=get_aucs("discretize5"), random=get_aucs("random"))
plot_AUCs(aucs)
```

